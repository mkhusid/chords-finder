{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/21 01:06:21 WARN Utils: Your hostname, Maxims-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)\n",
      "25/01/21 01:06:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/21 01:06:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/21 01:06:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark: SparkSession = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredStreaming\") \\\n",
    "    .config('spark.executor.memory', '16g') \\\n",
    "    .config('spark.driver.memory', '16g') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+--------------------+---------+-----------+--------------------+\n",
      "|          timestamp|          chords|              frames|user_name|record_name|            features|\n",
      "+-------------------+----------------+--------------------+---------+-----------+--------------------+\n",
      "|2025-01-20 10:26:46|['Am', 'F', 'C']|['1.67', '4.09', ...|test_user|       test|{'tempo': 99.3840...|\n",
      "|2025-01-20 10:26:47|['Am', 'F', 'C']|['1.67', '4.09', ...|test_user|       test|{'tempo': 99.3840...|\n",
      "+-------------------+----------------+--------------------+---------+-----------+--------------------+\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- chords: string (nullable = true)\n",
      " |-- frames: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- record_name: string (nullable = true)\n",
      " |-- features: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell is for setting up the streaming data of recorded chords and features.\n",
    "    1. Data is read from a csv file and streamed in batches of 1 file per trigger.\n",
    "    2. The data is then grouped by record_name, user_name and a 5 second window.\n",
    "    3. The chords and features are aggregated by taking the last value in the window.\n",
    "    4. Chords and features are taken by the last values in the window.\n",
    "'''\n",
    "\n",
    "basepath = '.'\n",
    "timestamp_format = 'MM/dd/yy HH:mm:ss'\n",
    "\n",
    "sample_df = spark.read.option(\"header\", \"true\").csv(f\"{basepath}/streamed_chords/test.csv\")\n",
    "schema = sample_df.schema\n",
    "\n",
    "sample_df = sample_df.withColumn('timestamp', F.to_timestamp(\"timestamp\", timestamp_format)).orderBy('timestamp')\n",
    "sample_df.show(5)\n",
    "\n",
    "input_stream = spark.readStream\\\n",
    "    .schema(schema)\\\n",
    "    .option('header', True)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .option(\"pathGlobFilter\", \"*.csv\")\\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(f\"{basepath}/streamed_chords/\")\\\n",
    "    .withColumn('timestamp', F.to_timestamp(\"timestamp\", timestamp_format))\\\n",
    "    .dropna().withWatermark(\"timestamp\", \"10 seconds\")\n",
    "\n",
    "  \n",
    "input_stream.printSchema() \n",
    "input_stream.writeStream.format(\"memory\").queryName('input_stream').start()\n",
    "       \n",
    "windowed_records = input_stream.groupBy(\n",
    "    input_stream.record_name,\n",
    "    input_stream.user_name,\n",
    "    F.window(input_stream.timestamp, '5 second')\n",
    ").agg(\n",
    "    F.last(\"chords\").alias(\"chords\"),\n",
    "    F.last(\"features\").alias(\"features\"),\n",
    "    F.last(\"timestamp\").alias(\"latest_timestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'artist_name', 'song_name', 'chords&lyrics', 'chords',\n",
      "       'lyrics', 'tabs', 'lang', 'artist_id', 'followers', 'genres',\n",
      "       'popularity', 'name_e_chords'],\n",
      "      dtype='object')\n",
      "root\n",
      " |-- song_name: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- chords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "135783"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' This cell is for basic data cleanup and transformation. '''\n",
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def data_cleanup(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(df.columns)\n",
    "\n",
    "    df = df[['song_name', 'artist_name', 'chords', 'genres']]\n",
    "\n",
    "    df['chords'] = list(map(lambda ch: ch.replace(\"'\",\"\").replace(',',' ').split(), df['chords']))\n",
    "    regex = r\"^(C|C#|D|D#|E|F|F#|G|G#|A|A#|B)(m)?$\"\n",
    "\n",
    "    for i, chords_list in enumerate(df['chords']):\n",
    "        clean_chords = [chord for chord in chords_list if re.match(regex, chord)]\n",
    "        df['chords'][i] = clean_chords\n",
    "        \n",
    "    df.to_csv('/Users/mkhusid/Desktop/Code/python/autochord/data/chords.csv', sep=';')\n",
    "    return df\n",
    "\n",
    "clean_chords_df = data_cleanup('./data/chords_and_lyrics.csv')\n",
    "chords_and_lyrics = spark.createDataFrame(clean_chords_df)\n",
    "chords_and_lyrics.printSchema()\n",
    "chords_and_lyrics.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "def parse_recorded_data(chords_data):\n",
    "  ''' \n",
    "  Parse the recorded chords and features data, then classify the genre of the song using pre-trained model.\n",
    "  Args:\n",
    "    chords_data: A DataFrame containing the recorded chords and features data.\n",
    "  Returns:\n",
    "    A DataFrame containing the classified genre of the song.\n",
    "  '''\n",
    "  \n",
    "  @F.udf('float')\n",
    "  def parse_feature(features, feature):\n",
    "      return ast.literal_eval(features)[feature]\n",
    "\n",
    "  users_chords = chords_data.withColumnRenamed('chords', 'compared')\\\n",
    "          .withColumn(\"tempo\", parse_feature(F.col('features'), F.lit('tempo')))\\\n",
    "          .withColumn(\"energy\", parse_feature(F.col('features'), F.lit('energy')))\\\n",
    "          .withColumn(\"acousticness\", parse_feature(F.col('features'), F.lit('acousticness')))\\\n",
    "          .withColumn(\"danceability\", parse_feature(F.col('features'), F.lit('danceability')))\\\n",
    "          .withColumn(\"liveness\", parse_feature(F.col('features'), F.lit('liveness')))\\\n",
    "          .withColumn(\"valence\", parse_feature(F.col('features'), F.lit('valence')))\\\n",
    "          .drop('features')    \n",
    "                \n",
    "  users_chords.printSchema()\n",
    "\n",
    "  genres_classifier = PipelineModel.load('./models/genres_model/')\n",
    "  predictions = genres_classifier.transform(users_chords)\n",
    "\n",
    "  @F.udf\n",
    "  def decoded(pred):\n",
    "    decoded_genres = ['rock', 'pop']\n",
    "    return decoded_genres[int(pred)]\n",
    "\n",
    "  recorded_classified = predictions.withColumn('genre', decoded(F.col('prediction'))).select('record_name', 'user_name', 'compared', 'genre')\n",
    "  recorded_classified.printSchema()\n",
    "  \n",
    "  return recorded_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- record_name: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- compared: string (nullable = true)\n",
      " |-- latest_timestamp: timestamp (nullable = true)\n",
      " |-- tempo: float (nullable = true)\n",
      " |-- energy: float (nullable = true)\n",
      " |-- acousticness: float (nullable = true)\n",
      " |-- danceability: float (nullable = true)\n",
      " |-- liveness: float (nullable = true)\n",
      " |-- valence: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- record_name: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- compared: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x29adc5ee0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This cell is to find similar songs within a input stream of recorded chords and features.\n",
    "It creates an output stream of similar songs with a similarity score greater than a threshold.\n",
    "This allows to implement real-time analysis of the input stream for all the users simultaneously.\n",
    "'''\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import ast\n",
    "\n",
    "@F.udf\n",
    "def jac_similarity(v1, v2):\n",
    "    ''' Define the Jaccard similarity function for two sets of chords. '''\n",
    "    set1 = set(v1)\n",
    "    set2 = set(ast.literal_eval(v2))\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union != 0 else 0.0\n",
    "\n",
    "@F.udf\n",
    "def check_genre(genre, genres_list):\n",
    "    return genre in genres_list \n",
    "\n",
    "def find_similar(original_df, input_stream, threshold):  \n",
    "    ''' Join original data with the input stream & calculate chords similarity. ''' \n",
    "    joined = original_df.crossJoin(input_stream)\n",
    "    result = joined.withColumn(\"similarity\", jac_similarity(F.col(\"chords\"), F.col(\"compared\")))\n",
    "    return result.filter(F.col(\"similarity\") > threshold)\n",
    "\n",
    "   \n",
    "recorded_classified = parse_recorded_data(windowed_records)\n",
    "filtered = find_similar(chords_and_lyrics, recorded_classified, 0.8).withColumn('genres_match', check_genre('genre', 'genres'))\n",
    "final = filtered.filter('genres_match == true').orderBy(F.asc('similarity'))\n",
    "final.writeStream.format(\"memory\").queryName('analyzer_results').outputMode(\"complete\").trigger(processingTime=\"5 seconds\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------+-------------+--------------------+----------------+-----+\n",
      "|user_name|record_name|        song_name|  artist_name|              chords|        recorded|genre|\n",
      "+---------+-----------+-----------------+-------------+--------------------+----------------+-----+\n",
      "|test_user|      test2|     Runaway Love|Justin Bieber|[Am, C, G, Am, C,...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|     test23|     Runaway Love|Justin Bieber|[Am, C, G, Am, C,...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|      test2|       Better Man| Taylor Swift|[C, C, G, C, G, G...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|     test23|       Better Man| Taylor Swift|[C, C, G, C, G, G...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|      test2|     Shake It Off| Taylor Swift|[Am, C, G, Am, C,...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|     test23|     Shake It Off| Taylor Swift|[Am, C, G, Am, C,...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|       test|     Sweet Escape| Taylor Swift|[Am, C, F, Am, C,...|['Am', 'F', 'C']|  pop|\n",
      "|test_user|       test|         Epiphany|          BTS|[F, C, Am, F, C, ...|['Am', 'F', 'C']|  pop|\n",
      "|test_user|       test|          Save Me|          BTS|[F, C, Am, F, C, ...|['Am', 'F', 'C']|  pop|\n",
      "|test_user|       test|          Happier|   Ed Sheeran|[F, C, F, C, F, C...|['Am', 'F', 'C']|  pop|\n",
      "|test_user|       test|        In Memory|   Ed Sheeran|[Am, F, C, Am, F,...|['Am', 'F', 'C']|  pop|\n",
      "|test_user|      test2|    Master Of War|   Ed Sheeran|[G, Am, Am, C, G,...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|     test23|    Master Of War|   Ed Sheeran|[G, Am, Am, C, G,...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|      test2|        New Flame|  Chris Brown|[C, C, Am, G, C, ...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|     test23|        New Flame|  Chris Brown|[C, C, Am, G, C, ...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|      test2|        One Thing|One Direction|[C, G, C, C, G, C...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|     test23|        One Thing|One Direction|[C, G, C, C, G, C...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|       test|        Right Now|One Direction|[F, C, F, Am, F, ...|['Am', 'F', 'C']|  pop|\n",
      "|test_user|      test2|Vas Happenin Boys|One Direction|[C, Am, G, C, C, ...|['C', 'G', 'Am']|  pop|\n",
      "|test_user|     test23|Vas Happenin Boys|One Direction|[C, Am, G, C, C, ...|['C', 'G', 'Am']|  pop|\n",
      "+---------+-----------+-----------------+-------------+--------------------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_prediticons_for_user(user_id):\n",
    "    ''' Get the predictions for a specific user. '''\n",
    "    columns = ['record_name', 'song_name', 'artist_name', 'chords', 'compared', 'genre']\n",
    "    query = f'select {\", \".join(columns)} from analyzer_results where user_name=\"{user_id}\"'\n",
    "    return spark.sql(query)\n",
    "\n",
    "user_id = input('Enter user name: ')\n",
    "get_prediticons_for_user(user_id).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chords_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
