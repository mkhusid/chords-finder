{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/20 18:38:36 WARN Utils: Your hostname, Maxims-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)\n",
      "25/01/20 18:38:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/20 18:38:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/20 18:38:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/01/20 18:38:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/01/20 18:38:38 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def start_spark_session(mode='client'):\n",
    "    if mode == 'cluster':\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"cluster-test-2\") \\\n",
    "            .master(\"spark://localhost:7077\") \\\n",
    "            .config('deploy-mode', 'cluster') \\\n",
    "            .config('spark.executor.memory', '2g') \\\n",
    "            .getOrCreate()\n",
    "    else:     \n",
    "        conf = SparkConf().setAll([('spark.executor.memory', '16g'),('spark.driver.memory','16g')])\n",
    "        sc = SparkContext(conf=conf)\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"localApp\") \\\n",
    "            .getOrCreate()\n",
    "            \n",
    "    return spark\n",
    "  \n",
    "spark = start_spark_session()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bafa4ad-6e47-4c7a-8d07-4009b98d84b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- duration_ms: string (nullable = true)\n",
      " |-- explicit: string (nullable = true)\n",
      " |-- danceability: string (nullable = true)\n",
      " |-- energy: string (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- loudness: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- speechiness: string (nullable = true)\n",
      " |-- acousticness: string (nullable = true)\n",
      " |-- instrumentalness: string (nullable = true)\n",
      " |-- liveness: string (nullable = true)\n",
      " |-- valence: string (nullable = true)\n",
      " |-- tempo: string (nullable = true)\n",
      " |-- time_signature: string (nullable = true)\n",
      " |-- track_genre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Unnamed: 0: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- duration_ms: string (nullable = true)\n",
      " |-- explicit: string (nullable = true)\n",
      " |-- danceability: string (nullable = true)\n",
      " |-- energy: string (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- loudness: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- speechiness: string (nullable = true)\n",
      " |-- acousticness: string (nullable = true)\n",
      " |-- instrumentalness: string (nullable = true)\n",
      " |-- liveness: string (nullable = true)\n",
      " |-- valence: string (nullable = true)\n",
      " |-- tempo: string (nullable = true)\n",
      " |-- time_signature: string (nullable = true)\n",
      " |-- track_genre: string (nullable = false)\n",
      "\n",
      "28997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "basepath = \"./data\"\n",
    "\n",
    "sp_tracks_genres = spark.read.format(\"csv\").option(\"header\", \"true\").load(\n",
    "  f\"{basepath}/sp_genres.csv\")\n",
    "\n",
    "sp_tracks_genres.printSchema()\n",
    "\n",
    "# Filter out unpopular genres\n",
    "genres_to_train = tuple(['rock', 'pop'])\n",
    "\n",
    "pop_genres = ['pop', 'happy', 'dance', 'disco']\n",
    "rock_genres = ['rock', 'metal', 'hardcore', 'alternative']\n",
    "\n",
    "sp_rocks_genres = sp_tracks_genres.filter(\n",
    "  \" or \".join([f\"track_genre like '%{g}%'\" for g in rock_genres])).withColumn('track_genre', F.lit('rock'))\n",
    "\n",
    "sp_pop_genres = sp_tracks_genres.filter(\n",
    "  \" or \".join([f\"track_genre like '%{g}%'\" for g in pop_genres])).withColumn('track_genre', F.lit('pop'))\n",
    "\n",
    "sp_tracks_genres = sp_rocks_genres.union(sp_pop_genres)\n",
    "\n",
    "sp_tracks_genres.printSchema()\n",
    "print(sp_tracks_genres.count())\n",
    "\n",
    "# genres_count = sp_tracks_genres.groupBy(\"track_genre\").count()\n",
    "# genres_count = genres_count.orderBy(F.desc('count')).filter(f'count >= 1000').select('track_genre').collect()\n",
    "# popular_genres = list(map(lambda c: c.track_genre, genres_count))\n",
    "# print(popular_genres)\n",
    "\n",
    "# sp_tracks_genres = sp_tracks_genres.filter(f'track_genre IN {tuple(popular_genres)}').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724e2193-7de9-48e9-bce4-46b053db4ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- track_genre: string (nullable = false)\n",
      " |-- energy: float (nullable = true)\n",
      " |-- tempo: float (nullable = true)\n",
      " |-- acousticness: float (nullable = true)\n",
      " |-- danceability: float (nullable = true)\n",
      " |-- liveness: float (nullable = true)\n",
      " |-- valence: float (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/20 18:38:43 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-------------------+-------------------+-------------------+\n",
      "|summary|            energy|             tempo|      acousticness|       danceability|           liveness|            valence|\n",
      "+-------+------------------+------------------+------------------+-------------------+-------------------+-------------------+\n",
      "|  count|             28997|             28997|             28997|              28997|              28997|              28997|\n",
      "|   mean|0.7204333030642077|124.76133466601458|0.2097648606088669| 0.5458733489925832|0.20694493438788245| 0.4990960238885881|\n",
      "| stddev|0.2121907471272912|30.088355642190862|0.2701534315384835|0.16050628602940753|0.16576629517657007|0.24580823078939482|\n",
      "|    min|            0.0167|            34.821|               0.0|             0.0513|            0.00986|             0.0166|\n",
      "|    max|               1.0|           219.571|             0.996|              0.975|                1.0|              0.993|\n",
      "+-------+------------------+------------------+------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "label_column = \"track_genre\"\n",
    "numerical_cols = [\"energy\", \"tempo\", \"acousticness\", \"danceability\", \"liveness\", \"valence\"]\n",
    "                  \n",
    "for column in numerical_cols:\n",
    "    sp_tracks_genres = sp_tracks_genres.withColumn(column, F.col(column).cast(FloatType()))\n",
    "\n",
    "tacks_model_data = sp_tracks_genres.select('track_id', 'track_genre', *numerical_cols)\n",
    "\n",
    "\n",
    "# Review the feature distributions and schema\n",
    "tacks_model_data.printSchema()\n",
    "tacks_model_data.select(*numerical_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Imports\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5477d61e-5a11-43ff-a373-52b869fb32c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кількість рядків у тренувальній вибірці: 23290\n",
      "Кількість рядків у тестовій вибірці: 5707\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Feature Engineering\n",
    "indexer = StringIndexer(inputCol='track_genre', outputCol='genre_id')\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features_assembled\")\n",
    "scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features_scaled\", withMean=True, withStd=True)\n",
    "\n",
    "# Тренувальна та тестова вибірки\n",
    "(train_data, test_data) = tacks_model_data.randomSplit([0.8, 0.2], seed=1234)\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "print(f\"Кількість рядків у тренувальній вибірці: {train_data.count()}\")\n",
    "print(f\"Кількість рядків у тестовій вибірці: {test_data.count()}\")\n",
    "\n",
    "# Multiclass evaluator to measure accuracy and F1\n",
    "evaluator_ml = MulticlassClassificationEvaluator(labelCol=\"genre_id\")\n",
    "cv_evaluator = MulticlassClassificationEvaluator(labelCol=\"genre_id\", metricName=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e3f0cc-7417-4fb7-8066-2452d0a49396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/20 18:38:49 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7292798317855266, f1: 0.7283670609960342\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Ініціалізація Random Forest класифікатора\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features_scaled\", \n",
    "    labelCol=\"genre_id\", \n",
    "    predictionCol=\"prediction\",\n",
    "    numTrees=50,  # Кількість дерев у лісі\n",
    "    maxDepth=5,   # Максимальна глибина дерев\n",
    "    seed=1234\n",
    ")\n",
    "\n",
    "\n",
    "# Create a Pipeline\n",
    "rf_pipeline = Pipeline(stages=[indexer, assembler, scaler, rf])\n",
    "\n",
    "# Тренування моделі на навчальній вибірці\n",
    "rf_model = rf_pipeline.fit(train_data)\n",
    "\n",
    "# Передбачення на тестовій вибірці\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Обчислення метрик для Random Forest\n",
    "rf_accuracy = evaluator_ml.setMetricName(\"accuracy\").evaluate(rf_predictions)\n",
    "rf_f1 = evaluator_ml.setMetricName(\"f1\").evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Accuracy: {rf_accuracy}, f1: {rf_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CrossValidatorModel' object has no attribute 'bestModelx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z_/8ww8jhrj3bb102d85v6ycx_c0000gn/T/ipykernel_31508/283759119.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Виконання K-fold валідації для Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcv_model_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModelx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mcv_rf_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_model_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CrossValidatorModel' object has no attribute 'bestModelx'"
     ]
    }
   ],
   "source": [
    "from src.utils import plot_roc\n",
    "\n",
    "# Побудова сітки гіперпараметрів для Random Forest\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5]) \\\n",
    "    .build()\n",
    "\n",
    "# CrossValidator для \n",
    "cv_rf = CrossValidator(estimator=rf_pipeline,\n",
    "            estimatorParamMaps=paramGrid_rf,\n",
    "            evaluator=cv_evaluator, \n",
    "            numFolds=5,\n",
    "            seed=1234, \n",
    "            parallelism=8)\n",
    "\n",
    "# Виконання K-fold валідації для Random Forest\n",
    "cv_model_rf = cv_rf.fit(train_data).bestModelx\n",
    "cv_rf_predictions = cv_model_rf.transform(test_data)\n",
    "\n",
    "# Обчислення метрик для Random Forest\n",
    "rf_accuracy = evaluator_ml.setMetricName(\"accuracy\").evaluate(cv_rf_predictions)\n",
    "print(f\"Best accuracy: {rf_accuracy}\")\n",
    "\n",
    "print(rf_model.stages[0])\n",
    "# plot_roc(rf_predictions, rf_model.stages[0])\n",
    "\n",
    "# Save model for further use in \"song_finder\" notebook\n",
    "cv_model_rf.save('./models/genres_model/')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3304139579577790,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "CP2 - Classification & Clusterization",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "chords_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
